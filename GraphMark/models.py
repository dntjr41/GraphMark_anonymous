import os
import torch
import re
import spacy
import time
import random
from typing import List, Dict, Optional
from transformers import BertTokenizer, BertModel
import numpy as np
from subgraph_construction import subgraph_construction
from transformers import RobertaModel, RobertaTokenizer
from llm_models import LLM, ChatGPT, Llama3_8B, Mistral7B, Qwen2_5_7B, KEPLEREmbedding

torch.manual_seed(42)
random.seed(42)

class KGWatermarker():
    def __init__(self, llm, ratio, topk=5, device_id=None, rarity_similarity_threshold: float = 0.6):
        if device_id is not None and torch.cuda.is_available():
            self.device = torch.device(f'cuda:{device_id}')
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        if isinstance(llm, str):
            self.llm = LLM(llm, device_id=device_id)
        else:
            self.llm = llm
        self.ratio = ratio
        self.rarity_similarity_threshold = rarity_similarity_threshold
        self.nlp = spacy.load("en_core_web_sm")
        self.roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
        self.roberta_model = RobertaModel.from_pretrained("roberta-base").to(self.device)
        self.roberta_model.eval()
        
        kg_root_path = "/home/wooseok/KG_Mark/kg/processed_wikidata5m"
        self.constructor = subgraph_construction(
            llm=self.llm, ratio=ratio, topk=topk,
            kg_entity_path=f"{kg_root_path}/entities.txt", 
            kg_relation_path=f"{kg_root_path}/relations.txt", 
            kg_triple_path=f"{kg_root_path}/triplets.txt", 
            device_id=device_id)
        
        self.entity, self.relation, self.triple = self.constructor.load_kg(
            f"{kg_root_path}/entities.txt", 
            f"{kg_root_path}/relations.txt", 
            f"{kg_root_path}/triplets.txt")

    def _get_sentence_embedding(self, text):
        """RoBERTa sentence embedding via [CLS] (start token) representation."""        
        encoded = self.roberta_tokenizer(
            text, return_tensors="pt", truncation=True,
            padding=True, max_length=256).to(self.device)
        with torch.no_grad():
            outputs = self.roberta_model(**encoded)
        # CLS-equivalent: first token hidden state
        cls_vec = outputs.last_hidden_state[:, 0, :].squeeze(0).detach().cpu().numpy()
        return cls_vec
    
    def _triplet_to_plain_sentence(self, triplet):
        """Create a simple textual representation of a triplet using KG labels."""
        if not isinstance(triplet, (list, tuple)) or len(triplet) < 3:
            return str(triplet)

        head, relation, tail = triplet[:3]
        head_name = self._select_best_name(head, self.entity)
        relation_name = self._select_best_name(relation, self.relation)
        tail_name = self._select_best_name(tail, self.entity)
        return f"{head_name} {relation_name} {tail_name}"

    def _filter_triplets_by_rarity(self, triplets, original_sentences, threshold=None, fallback_keep: int = None):
        """Select only triplets that are semantically distant from original sentences."""
        if not triplets:
            return triplets, []
        
        if threshold is None:
            threshold = getattr(self, 'rarity_similarity_threshold', 0.6)
        
        # Fallback keep ê°œìˆ˜ë¥¼ ratio ê¸°ë°˜ìœ¼ë¡œ ì¡°ì • (ìµœì†Œ 5ê°œ)
        if fallback_keep is None:
            fallback_keep = max(5, int(len(triplets) * 0.3))  # ìµœì†Œ 30% ë˜ëŠ” 5ê°œ

        # Prepare sentence embeddings once
        sentence_embeddings = []
        if original_sentences:
            for sent in original_sentences:
                try:
                    emb = self._get_sentence_embedding(sent)
                    if emb is not None:
                        sentence_embeddings.append((sent, emb))
                except Exception:
                    continue

        if not sentence_embeddings:
            return triplets, []

        accepted = []
        scored_triplets = []

        for triplet in triplets:
            try:
                triplet_sentence = self._triplet_to_plain_sentence(triplet)
                triplet_emb = self._get_sentence_embedding(triplet_sentence)
                if triplet_emb is None:
                    continue

                similarities = [self._calculate_cosine_similarity(triplet_emb, sent_emb) for _, sent_emb in sentence_embeddings]
                max_sim = max(similarities) if similarities else 0.0

                scored_triplets.append((triplet, max_sim, triplet_sentence))

                if max_sim < threshold:
                    accepted.append((triplet, max_sim, triplet_sentence))
            except Exception:
                continue

        if accepted:
            return [triplet for triplet, _, _ in accepted], accepted

        # Fallback: keep the lowest similarity triplets to avoid empty selection
        # ë” ë§ì€ tripletì„ ë³´ì¡´í•˜ì—¬ ratio ê¸°ë°˜ ì„ íƒì´ ê°€ëŠ¥í•˜ë„ë¡
        scored_triplets.sort(key=lambda item: item[1])
        fallback_count = min(fallback_keep, len(scored_triplets))
        fallback = scored_triplets[:fallback_count]
        print(f"   âš ï¸ Rarity filtering: No triplets below threshold {threshold:.2f}, keeping {fallback_count} lowest similarity triplets")
        return [triplet for triplet, _, _ in fallback], fallback
    
    def build_subgraph_from_text(self, text, enable_adaptive_pruning=True, pruning_ratio=0.3):
        """Build subgraph from text using constructor"""
        return self.constructor.build_subgraph_from_text(text, enable_adaptive_pruning, pruning_ratio)
    
    def convert_triple_to_sentence(self, triple, keywords=None):
        head, relation, tail = triple
        head_name = self._select_best_name(head, self.entity, keywords)
        tail_name = self._select_best_name(tail, self.entity, keywords)
        relation_name = self._select_best_name(relation, self.relation)
        return self._create_fallback_sentence(head_name, relation_name, tail_name)
    
    def _select_best_name(self, item_id, data_dict, keywords=None):
        try:
            # 1. ë°ì´í„°ì—ì„œ ì´ë¦„ ëª©ë¡ ì¶”ì¶œ
            if item_id not in data_dict:
                return str(item_id)
            
            entity_data = data_dict[item_id]
            if isinstance(entity_data, dict):
                names = entity_data.get("entity" if "entity" in entity_data else "name", [str(item_id)])
            else:
                names = entity_data if isinstance(entity_data, list) else [str(item_id)]
            
            if not names:
                return str(item_id)
            
            # 2. í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° í‚¤ì›Œë“œì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì´ë¦„ ìš°ì„  ì„ íƒ
            if keywords:
                for keyword in keywords:
                    for name in names:
                        if isinstance(name, str) and keyword.lower() == name.lower():
                            return keyword
                    for name in names:
                        if isinstance(name, str) and (keyword.lower() in name.lower() or name.lower() in keyword.lower()):
                            return keyword
            
            # 3. ì˜ì–´ ì´ë¦„ ìš°ì„  í•„í„°ë§
            english_names = [name for name in names if isinstance(name, str) and self._is_english_text(name)]
            candidate_names = english_names if english_names else [name for name in names if isinstance(name, str)]
            
            if not candidate_names:
                return str(item_id)
            
            # 4. í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ì´ë¦„ ì„ íƒ
            best_name = candidate_names[0]
            for name in candidate_names:
                if self._is_better_name(name, best_name):
                    best_name = name
            
            return best_name
            
        except Exception as e:
            print(f"Error in _select_best_name: {e}")
            return str(item_id)
    
    def _is_english_text(self, text):
        """í…ìŠ¤íŠ¸ê°€ ì˜ì–´ì¸ì§€ í™•ì¸"""
        if not text or not isinstance(text, str):
            return False
        english_chars = sum(1 for c in text if c.isascii() and c.isalpha())
        total_chars = sum(1 for c in text if c.isalpha())
        if total_chars == 0:
            return False
        english_ratio = english_chars / total_chars
        return english_ratio >= 0.8
    
    def _is_better_name(self, name1, name2):
        """ë‘ ì´ë¦„ ì¤‘ ë” ë‚˜ì€ ì´ë¦„ ì„ íƒ (ì˜ì–´ ìš°ì„ , ì ì ˆí•œ ê¸¸ì´, íŠ¹ìˆ˜ë¬¸ì ìµœì†Œí™”)"""
        # 1. ì˜ì–´ ìš°ì„ 
        is_english1 = self._is_english_text(name1)
        is_english2 = self._is_english_text(name2)
        if is_english1 and not is_english2:
            return True
        if is_english2 and not is_english1:
            return False
        
        # 2. ì ì ˆí•œ ê¸¸ì´ (5-50ì) ìš°ì„ 
        len1, len2 = len(name1), len(name2)
        if 5 <= len1 <= 50 and not (5 <= len2 <= 50):
            return True
        if 5 <= len2 <= 50 and not (5 <= len1 <= 50):
            return False
        
        # 3. íŠ¹ìˆ˜ë¬¸ì ìµœì†Œí™”
        special1 = sum(1 for c in name1 if not c.isalnum() and c != ' ')
        special2 = sum(1 for c in name2 if not c.isalnum() and c != ' ')
        if special1 < special2:
            return True
        if special2 < special1:
            return False
        
        # 4. ì ì ˆí•œ ë‹¨ì–´ ìˆ˜ (1-5ê°œ) ìš°ì„ 
        words1, words2 = len(name1.split()), len(name2.split())
        if 1 <= words1 <= 5 and not (1 <= words2 <= 5):
            return True
        if 1 <= words2 <= 5 and not (1 <= words1 <= 5):
            return False
        
        # 5. ê¸¸ì´ê°€ ì§§ì€ ê²ƒ ìš°ì„ 
        return len1 < len2
    
    def _is_entity_name_match(self, entity_id, keyword):
        """ì—”í‹°í‹° IDì˜ ì´ë¦„ì´ í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸"""
        try:
            if entity_id not in self.entity:
                return False
                
            entity_data = self.entity[entity_id]
            
            # ì—”í‹°í‹° ë°ì´í„°ì—ì„œ ì´ë¦„ ëª©ë¡ ì¶”ì¶œ
            if isinstance(entity_data, dict):
                entity_names = entity_data.get('name', list(entity_data.values()))
            else:
                entity_names = entity_data
            
            # ì´ë¦„ ëª©ë¡ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            if isinstance(entity_names, list):
                for name in entity_names:
                    if isinstance(name, str):
                        if keyword.lower() == name.lower():
                            return True
                        if keyword.lower() in name.lower() or name.lower() in keyword.lower():
                            return True
                return False
            
            # ì´ë¦„ì´ ë¬¸ìì—´ì¸ ê²½ìš°
            elif isinstance(entity_names, str):
                if keyword.lower() == entity_names.lower():
                    return True
                if keyword.lower() in entity_names.lower() or entity_names.lower() in keyword.lower():
                    return True
                return False
            
            return False
            
        except Exception as e:
            print(f"Error in entity name matching: {e}")
            return False
    
    def insert_watermark(self, prefix, target, enable_adaptive_pruning=True, pruning_ratio=0.2):
        """Main watermarking function with improved LLM-based modification and insertion"""
        combined_text = f"{prefix} {target}"
        
        # Split text into sentences FIRST (needed for rarity filtering)
        doc = self.nlp(combined_text)
        sentences = [sent.text.strip() for sent in doc.sents]
        original_sentences = sentences.copy()  # Reference to original
        
        # Build subgraph
        subgraph_info = self.build_subgraph_from_text(combined_text, enable_adaptive_pruning, pruning_ratio)
        
        # Select triplets (now sentences is defined)
        # ë” ë§ì€ tripletì„ ì„ íƒí•˜ê¸° ìœ„í•´ í•„í„°ë§ ì„ê³„ê°’ì„ ì™„í™”
        selected_triplets, _, _ = self.select_triplets_for_watermarking(
            subgraph_info['subgraph_triples'], subgraph_info['keywords'], 
            main_topic=subgraph_info.get('main_topic', None), similarity_threshold=0.2,  # 0.3 -> 0.1ë¡œ ì™„í™”
            original_sentences=sentences,
            rarity_threshold=0.75  # 0.6 -> 0.8ë¡œ ì™„í™” (ë†’ì„ìˆ˜ë¡ ë” ë§ì€ triplet í†µê³¼)
        )
        
        # total_tripletsëŠ” subgraph_triplesì˜ ê°œìˆ˜
        total_triplets = len(subgraph_info['subgraph_triples'])
        selected_triplets_count = len(selected_triplets)
        
        # Adaptive Ratio ì ìš©: ë¬¸ì„œ ë³µì¡ë„ì— ë”°ë¥¸ Modify/Insert ë¹„ìœ¨ ì¡°ì •
        complexity_score = self._analyze_document_complexity(sentences)
        modify_ratio, insert_ratio = self._calculate_adaptive_ratio(complexity_score)
        
        # ìˆ˜ì • ë¹„ìœ¨ ì œí•œ (ratio ê¸°ë°˜, min: 3, max: 10 or 50% of sentences)
        # total_tripletsì˜ ratioë§Œí¼ ì‚¬ìš© (ì˜ˆ: 25ê°œ * 0.25 = 6.25 -> 6ê°œ, ìµœì†Œ 3ê°œ)
        ratio_based_count = int(total_triplets * self.ratio) if total_triplets > 0 else 0
        # ë¬¸ì¥ ìˆ˜ ê¸°ë°˜ ê³„ì‚° (ë¬¸ì¥ ìˆ˜ì˜ ratioë§Œí¼)
        sentence_based_count = int(len(sentences) * self.ratio)
        
        # ìµœì†Œê°’: 3 (ë‹¨, selected_triplets_countê°€ 3 ë¯¸ë§Œì´ë©´ selected_triplets_count)
        min_triplets = min(3, selected_triplets_count) if selected_triplets_count > 0 else 0
        # ìµœëŒ€ê°’: 10 ë˜ëŠ” ë¬¸ì¥ ìˆ˜ì˜ 50%
        max_triplets = min(10, int(len(sentences) * 0.5))
        
        # Ratio ê¸°ë°˜ ëª©í‘œ ê°œìˆ˜ (total_triplets ratioì™€ sentence ratio ì¤‘ ë” í° ê°’, ìµœì†Œ 3)
        target_by_ratio = max(ratio_based_count, sentence_based_count)
        target_by_ratio = max(min_triplets, target_by_ratio)  # ìµœì†Œê°’ ë³´ì¥
        target_by_ratio = min(target_by_ratio, max_triplets, selected_triplets_count)  # ìµœëŒ€ê°’ ë° ì„ íƒ ê°€ëŠ¥ ê°œìˆ˜ ì œí•œ
        
        max_allowed_triplets = target_by_ratio
        
        # ì‹¤ì œ ì‚¬ìš©í•  triplet ìˆ˜ ì œí•œ
        actual_triplets = selected_triplets[:max_allowed_triplets]
        modify_count = max(1, int(len(actual_triplets) * modify_ratio))
        insert_count = len(actual_triplets) - modify_count
        
        print(f"ğŸ“Š Document Complexity Analysis:")
        print(f"   Complexity Score: {complexity_score:.3f}")
        print(f"   Total Triplets (Subgraph): {total_triplets}")
        print(f"   Selected Triplets: {selected_triplets_count}")
        print(f"   Ratio-based count: {ratio_based_count} (from {total_triplets} * {self.ratio:.2f})")
        print(f"   Sentence-based count: {sentence_based_count} (from {len(sentences)} * {self.ratio:.2f})")
        print(f"   Allowed Triplets: {max_allowed_triplets} (min: {min_triplets}, max: {max_triplets} or 50% of sentences)")
        print(f"   Modify Ratio: {modify_ratio:.3f} ({modify_count} triplets)")
        print(f"   Insert Ratio: {insert_ratio:.3f} ({insert_count} triplets)")
        
        print(f"ğŸ“Š Triplet distribution: {len(actual_triplets)} used -> {modify_count} modify, {insert_count} insert")
        
        # Triplet ì„ íƒ ê¸°ì¤€ì— ë”°ë¼ Modify/Insert ë¶„ë¥˜
        modify_triplets, insert_triplets = self._select_triplets_for_modify_and_insert(
            actual_triplets, sentences, subgraph_info['keywords'], modify_count, insert_count
        )
        
        print(f"   Modify triplets: {len(modify_triplets)}")
        print(f"   Insert triplets: {len(insert_triplets)}")
        
        # Step 1: Modify sentences with selected triplets
        print(f"\n{'='*80}")
        print(f"STEP 1: MODIFY - Starting with {len(sentences)} original sentences")
        print(f"{'='*80}\n")
        
        modified_sentences, used_triplets = self.modify_sentences_with_keywords_and_triplets(
            sentences, modify_triplets, subgraph_info['keywords']
        )
        
        print(f"\nâœ… MODIFY complete: {len(modified_sentences)} sentences (original: {len(sentences)})")
        
        # ì›ë³¸ ê°œìˆ˜ ë³´ì¥ ê²€ì¦
        if len(modified_sentences) != len(sentences):
            print(f"âš ï¸  Warning: Sentence count changed during modification!")
            print(f"   Original: {len(sentences)}, Modified: {len(modified_sentences)}")
        
        # Step 2: Insert remaining triplets as new sentences (Modifyì—ì„œ ì‚¬ìš©ëœ ê²ƒ ì œì™¸)
        remaining_for_insert = [t for t in insert_triplets if tuple(t) not in used_triplets]
        if remaining_for_insert:
            print(f"\n{'='*80}")
            print(f"STEP 2: INSERT - Starting with {len(modified_sentences)} sentences")
            print(f"{'='*80}\n")
            
            watermarked_sentences = self.insert_sentences_at_appropriate_positions(
                modified_sentences, remaining_for_insert, subgraph_info['keywords']
            )
            
            print(f"\nâœ… INSERT complete: {len(watermarked_sentences)} total sentences")
        else:
            watermarked_sentences = modified_sentences
        
        # ì›ë³¸ ë¬¸ì¥ ë³´ì¡´ì„ ìœ„í•œ ì¤‘ë³µ ì œê±°
        print(f"ğŸ” Checking for duplicate sentences...")
        print(f"   Before duplicate removal: {len(watermarked_sentences)} sentences")
        watermarked_sentences = self._remove_duplicate_sentences_preserving_originals(
            watermarked_sentences, original_sentences
        )
        print(f"   After duplicate removal: {len(watermarked_sentences)} sentences")
        
        # Step 2.5: Verify and fix naturalness of sentences
        print(f"\n{'='*80}")
        print(f"STEP 2.5: NATURALNESS VERIFICATION - Checking sentence naturalness")
        print(f"{'='*80}\n")
        
        watermarked_sentences = self._verify_and_fix_naturalness(
            watermarked_sentences, original_sentences, subgraph_info['keywords']
        )
        
        print(f"\nâœ… NATURALNESS VERIFICATION complete: {len(watermarked_sentences)} sentences")
        
        # Combine results
        watermarked_text = " ".join(watermarked_sentences)
        
        # Step 3: Verify entity preservation and retry if needed (Iterative)
        print(f"\n{'='*80}")
        print(f"STEP 3: VERIFICATION - Checking entity preservation")
        print(f"{'='*80}\n")
        
        max_retries = 5
        retry_count = 0
        verified_triplets = actual_triplets.copy()
        
        while retry_count < max_retries:
            verification_results = self._verify_triplet_entity_preservation(
                watermarked_text, verified_triplets
            )
            
            # Count successfully preserved triplets
            preserved_count = sum(1 for v in verification_results.values() if v["both_found"])
            total_to_verify = len(verified_triplets)
            preservation_rate = preserved_count / total_to_verify if total_to_verify > 0 else 0.0
            
            print(f"   Verification attempt {retry_count + 1}: {preserved_count}/{total_to_verify} triplets preserved ({preservation_rate*100:.1f}%)")
            
            # If preservation rate is acceptable (>= 50%) or all preserved, break
            if preservation_rate >= 0.8 or preserved_count == total_to_verify:
                print(f"   âœ… Entity preservation verified: {preserved_count}/{total_to_verify} triplets")
                break
            
            # Find failed triplets (not preserved) - convert tuple back to list
            failed_triplets = [list(t) for t, v in verification_results.items() if not v["both_found"]]
            
            if not failed_triplets or retry_count >= max_retries - 1:
                print(f"   âš ï¸  Some triplets not fully preserved after {retry_count + 1} attempts")
                if failed_triplets:
                    print(f"   Failed triplets: {len(failed_triplets)}")
                break
            
            # Retry: Re-insert failed triplets
            print(f"   ğŸ”„ Retrying {len(failed_triplets)} failed triplets...")
            retry_count += 1
            
            # Re-insert failed triplets
            retry_inserted = self._retry_insert_failed_triplets(
                watermarked_sentences, failed_triplets, subgraph_info['keywords']
            )
            
            if retry_inserted:
                watermarked_sentences = retry_inserted
                watermarked_text = " ".join(watermarked_sentences)
                print(f"   âœ… Retry insertion complete: {len(watermarked_sentences)} sentences")
            else:
                print(f"   âš ï¸  Retry insertion failed, keeping current result")
                break
        
        # ê¸¸ì´ ì¦ê°€ ëª¨ë‹ˆí„°ë§
        original_length = len(combined_text)
        watermarked_length = len(watermarked_text)
        length_increase_ratio = (watermarked_length - original_length) / original_length if original_length > 0 else 0
        
        # ê¸¸ì´ ì¦ê°€ê°€ 50%ë¥¼ ì´ˆê³¼í•˜ë©´ ê²½ê³  (ë” ìœ ì—°í•œ ê¸°ì¤€)
        if length_increase_ratio > 0.5:
            print(f"   âš ï¸  Warning: Document length increased by {length_increase_ratio:.1%} (max recommended: 50%)")
        elif length_increase_ratio > 0.3:
            print(f"   â„¹ï¸  Info: Document length increased by {length_increase_ratio:.1%}")
        
        # í†µê³„ ê³„ì‚°
        actual_modified = sum(1 for i, (orig, mod) in enumerate(zip(sentences, modified_sentences)) if orig != mod)
        actual_inserted = len(insert_triplets)
        
        # modification_ratioì™€ insertion_ratioì˜ í•©ì´ 100ì´ ë˜ë„ë¡ ê³„ì‚°
        total_watermark_operations = actual_modified + actual_inserted
        if total_watermark_operations > 0:
            modification_ratio = (actual_modified / total_watermark_operations) * 100
            insertion_ratio = (actual_inserted / total_watermark_operations) * 100
        else:
            modification_ratio = 0.0
            insertion_ratio = 0.0
        
        return {
            "original_text": combined_text,
            "watermarked_text": watermarked_text,
            "keywords": subgraph_info['keywords'],
            "ratio": self.ratio,
            "total_triplets": total_triplets,
            "used_triplets": len(actual_triplets),
            "planned_modify": modify_count,
            "planned_insert": insert_count,
            "actual_modified_sentences": actual_modified,
            "actual_inserted_sentences": actual_inserted,
            "modification_ratio": modification_ratio,
            "insertion_ratio": insertion_ratio,
            "length_increase_ratio": length_increase_ratio,
            "original_length": original_length,
            "watermarked_length": watermarked_length,
            "subgraph_triples": subgraph_info['subgraph_triples'],
            "selected_triplets": actual_triplets
        }
    
    def _calculate_cosine_similarity(self, vec1, vec2):
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np
        
        # ë²¡í„° ì •ê·œí™”
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = np.dot(vec1, vec2) / (norm1 * norm2)
        return float(similarity)
    
    def _analyze_document_complexity(self, sentences):
        """ë¬¸ì„œì˜ ë³µì¡ë„ ë¶„ì„"""
        if not sentences:
            return 0.5  # ê¸°ë³¸ê°’
        
        total_chars = 0
        total_words = 0
        total_sentences = len(sentences)
        complex_sentences = 0
        
        for sentence in sentences:
            # ê¸°ë³¸ í†µê³„
            char_count = len(sentence)
            word_count = len(sentence.split())
            total_chars += char_count
            total_words += word_count
            
            # ë³µì¡ë„ ì§€í‘œë“¤
            is_complex = False
            
            # 1. ê¸¸ì´ ê¸°ì¤€ (ë¬¸ì ìˆ˜)
            if char_count > 100:  # 100ì ì´ìƒ
                is_complex = True
            
            # 2. ë‹¨ì–´ ìˆ˜ ê¸°ì¤€
            if word_count > 15:  # 15ë‹¨ì–´ ì´ìƒ
                is_complex = True
            
            # 3. ë¬¸ë²•ì  ë³µì¡ë„ (spaCy ë¶„ì„)
            try:
                doc = self.nlp(sentence)
                
                # ë³µí•©ë¬¸ (ì—°ê²°ì‚¬ê°€ ìˆëŠ” ê²½ìš°)
                conjunctions = [token for token in doc if token.pos_ == 'CCONJ']
                if len(conjunctions) > 0:
                    is_complex = True
                
                # ë¶€ì‚¬ì ˆ (ê´€ê³„ì‚¬ê°€ ìˆëŠ” ê²½ìš°)
                relative_clauses = [token for token in doc if token.dep_ in ['relcl', 'acl']]
                if len(relative_clauses) > 0:
                    is_complex = True
                
                # êµ¬ë‘ì  ë³µì¡ë„ (ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡  ë“±)
                punctuation_count = sum(1 for c in sentence if c in ';,:-')
                if punctuation_count > 2:
                    is_complex = True
                    
            except Exception:
                # spaCy ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸¸ì´ë§Œìœ¼ë¡œ íŒë‹¨
                pass
            
            if is_complex:
                complex_sentences += 1
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        avg_chars = total_chars / total_sentences if total_sentences > 0 else 0
        avg_words = total_words / total_sentences if total_sentences > 0 else 0
        complex_ratio = complex_sentences / total_sentences if total_sentences > 0 else 0
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë³µì¡ë„ ê³„ì‚°
        complexity_score = (
            min(avg_chars / 150, 1.0) * 0.4 +  # ë¬¸ì ìˆ˜ (150ì ê¸°ì¤€)
            min(avg_words / 20, 1.0) * 0.3 +   # ë‹¨ì–´ ìˆ˜ (20ë‹¨ì–´ ê¸°ì¤€)
            complex_ratio * 0.3                # ë³µì¡í•œ ë¬¸ì¥ ë¹„ìœ¨
        )
        
        return min(complexity_score, 1.0)
    
    def _calculate_adaptive_ratio(self, complexity_score):
        """ë³µì¡ë„ì— ë”°ë¥¸ Modify/Insert ë¹„ìœ¨ ê³„ì‚°"""
        # ë³µì¡ë„ê°€ ë‚®ìœ¼ë©´ (ì§§ê³  ê°„ê²°) â†’ Insert < Modify (0.3:0.7)
        # ë³µì¡ë„ê°€ ë†’ìœ¼ë©´ (ê¸¸ê³  ë³µì¡) â†’ Insert > Modify (0.7:0.3)
        
        # ë³µì¡ë„ 0.0 ~ 1.0ì„ 0.3 ~ 0.7ë¡œ ë§¤í•‘
        insert_ratio = 0.3 + (complexity_score * 0.4)
        modify_ratio = 1.0 - insert_ratio
        
        return modify_ratio, insert_ratio
    
    
    
    def _calculate_triplet_topic_similarity(self, triple, topic_embedding):
        """íŠ¸ë¦¬í”Œë ›ê³¼ ë©”ì¸ í† í”½ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (enhanced for better theme matching)"""
        if topic_embedding is None:
            return 1.0  # í† í”½ì´ ì—†ìœ¼ë©´ ëª¨ë“  íŠ¸ë¦¬í”Œë › í—ˆìš©
        
        # Tripletì˜ entity nameë“¤ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ìœ ì‚¬ë„ ê³„ì‚°
        try:
            h, r, t = triple
            
            # Entityì™€ relation name ê°€ì ¸ì˜¤ê¸°
            h_name = self._select_best_name(h, self.entity)
            t_name = self._select_best_name(t, self.entity)
            r_name = self._select_best_name(r, self.relation)
            
            # Tripleì„ ê°„ë‹¨í•œ ë¬¸ì¥ìœ¼ë¡œ í‘œí˜„
            if h_name and t_name and r_name:
                triple_text = f"{h_name} {r_name} {t_name}"
            else:
                triple_text = self.convert_triple_to_sentence(triple)
            
            if not triple_text:
                return 0.0
            
            sentence_embedding = self._get_sentence_embedding(triple_text)
            if sentence_embedding is None:
                return 0.0
            
            similarity = self._calculate_cosine_similarity(sentence_embedding, topic_embedding)
            return similarity
        except Exception as e:
            print(f"Error calculating triplet similarity: {e}")
            return 0.0
    
    def select_triplets_for_watermarking(self, subgraph_triples, keywords, keyword_triplets=None, main_topic=None,
                                         similarity_threshold=0.2, original_sentences=None, rarity_threshold=0.75):
        """Select triplets for watermarking based on keywords"""
        if not subgraph_triples:
            return [], 0, []
        
        triplets_list = list(subgraph_triples.values()) if isinstance(subgraph_triples, dict) else subgraph_triples
        
        # ë©”ì¸ í† í”½ ì„ë² ë”© ê³„ì‚° (ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        topic_embedding = None
        if main_topic:
            topic_embedding = self._get_sentence_embedding(main_topic)
        
        # í† í”½ ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§ (optional - lenient)
        filtered_triplets = []
        if topic_embedding is not None:
            for triple in triplets_list:
                similarity = self._calculate_triplet_topic_similarity(triple, topic_embedding)
                if similarity >= similarity_threshold:
                    filtered_triplets.append(triple)
        else:
            filtered_triplets = triplets_list
        
        print(f"ğŸ“Š Theme filtering: {len(triplets_list)} -> {len(filtered_triplets)} triplets (threshold: {similarity_threshold if topic_embedding else 'N/A'})")
        
        # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•„í„°ë§ ê±´ë„ˆë›°ê¸°
        if len(filtered_triplets) < len(triplets_list) * 0.3:  # 30% ë¯¸ë§Œì´ë©´ í•„í„°ë§ ê±´ë„ˆë›°ê¸°
            print(f"   âš ï¸ Too few triplets after theme filtering, using all triplets")
            filtered_triplets = triplets_list
        
        rarity_threshold = rarity_threshold if rarity_threshold is not None else 0.75
        rarity_filtered_triplets, rarity_details = self._filter_triplets_by_rarity(
            filtered_triplets, original_sentences, threshold=rarity_threshold
        )

        if rarity_details:
            rare_count = sum(1 for _, sim, _ in rarity_details if sim < rarity_threshold)
            print(f"ğŸ” Rarity filtering: kept {len(rarity_filtered_triplets)}/{len(filtered_triplets)} triplets below similarity {rarity_threshold:.2f}")
            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•„í„°ë§ ê±´ë„ˆë›°ê¸°
            if len(rarity_filtered_triplets) < len(filtered_triplets) * 0.3:  # 30% ë¯¸ë§Œì´ë©´ í•„í„°ë§ ê±´ë„ˆë›°ê¸°
                print(f"   âš ï¸ Too few triplets after rarity filtering, using theme-filtered triplets")
                rarity_filtered_triplets = filtered_triplets
            elif rare_count == 0:
                closest = ', '.join([
                    f"{trip[:3]} (sim={sim:.2f})" for trip, sim, _ in rarity_details[:3]
                ])
                print(f"   âš ï¸ No triplets below threshold; using closest candidates: {closest}")
        else:
            print(f"ğŸ” Rarity filtering skipped (no embeddings available)")

        filtered_triplets = rarity_filtered_triplets
        
        # Ratio ê¸°ë°˜ triplet ì„ íƒ (ë” ê´€ëŒ€í•˜ê²Œ)
        # ì›ë³¸ triplets_listì˜ ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ratio ì ìš©, í•˜ì§€ë§Œ filtered_triplets ë²”ìœ„ ë‚´ì—ì„œ
        base_count = len(triplets_list)  # ì›ë³¸ ì „ì²´ ê°œìˆ˜
        ratio_based_count = max(3, int(base_count * self.ratio))  # Ratio ê¸°ë°˜ ëª©í‘œ ê°œìˆ˜
        
        # Filtered tripletsê°€ ì¶©ë¶„í•˜ë©´ ratio ê¸°ë°˜ ê°œìˆ˜ ì‚¬ìš©, ë¶€ì¡±í•˜ë©´ ê°€ëŠ¥í•œ ë§Œí¼ ì‚¬ìš©
        min_count = min(3, len(filtered_triplets))
        max_count = len(filtered_triplets)  # í•„í„°ë§ëœ ëª¨ë“  triplet ì‚¬ìš© ê°€ëŠ¥
        target_count = max(min_count, min(max_count, ratio_based_count))
        
        # í‚¤ì›Œë“œë³„ë¡œ triplet ë¶„ë¥˜
        keyword_triplets_dict = {}
        other_triplets = []
        
        if filtered_triplets:
            for triple in filtered_triplets:
                if self._is_meaningful_triplet(triple, keywords):
                    # ì–´ë–¤ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœì§€ ì°¾ê¸°
                    matched_keyword = None
                    for keyword in keywords:
                        if self._is_triplet_related_to_keyword(triple, keyword):
                            if keyword not in keyword_triplets_dict:
                                keyword_triplets_dict[keyword] = []
                            keyword_triplets_dict[keyword].append(triple)
                            matched_keyword = keyword
                            break
                    
                    if not matched_keyword:
                        other_triplets.append(triple)
        
        # í‚¤ì›Œë“œë³„ë¡œ ìµœì†Œ 1ê°œì”© ì„ íƒ
        selected_triplets = []
        used_triplets = set()
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ ìµœì†Œ 1ê°œì”© ì„ íƒ
        for keyword in keywords:
            if keyword in keyword_triplets_dict and keyword_triplets_dict[keyword]:
                # í•´ë‹¹ í‚¤ì›Œë“œì˜ ì²« ë²ˆì§¸ triplet ì„ íƒ
                selected_triplet = keyword_triplets_dict[keyword][0]
                selected_triplets.append(selected_triplet)
                used_triplets.add(tuple(selected_triplet))
                print(f"   ğŸ¯ Selected triplet for keyword '{keyword}': {selected_triplet}")
        
        # ë‚¨ì€ tripletë“¤ ì¤‘ì—ì„œ ì¶”ê°€ ì„ íƒ (filtered_tripletsì—ì„œ ê°€ì ¸ì˜´)
        remaining_triplets = [t for t in filtered_triplets if tuple(t) not in used_triplets]
        
        # ëª©í‘œ ê°œìˆ˜ê¹Œì§€ ì¶”ê°€ ì„ íƒ
        while len(selected_triplets) < target_count and remaining_triplets:
            selected_triplets.append(remaining_triplets.pop(0))
        
        print(f"ğŸ” Selected {len(selected_triplets)}/{target_count} triplets for watermarking")
        print(f"   - Keyword-related: {len([t for t in selected_triplets if any(self._is_triplet_related_to_keyword(t, k) for k in keywords)])}")
        print(f"   - Other: {len(selected_triplets) - len([t for t in selected_triplets if any(self._is_triplet_related_to_keyword(t, k) for k in keywords)])}")
        
        return selected_triplets, len(triplets_list), list(range(len(selected_triplets)))
    
    def _is_triplet_related_to_keyword(self, triple, keyword):
        """Tripletì´ íŠ¹ì • í‚¤ì›Œë“œì™€ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸"""
        head, relation, tail = triple
        
        # Headë‚˜ Tailì´ í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
        head_matches = self._is_entity_name_match(head, keyword)
        tail_matches = self._is_entity_name_match(tail, keyword)
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶œë ¥
        if head_matches or tail_matches:
            print(f"   âœ… Triplet {triple} matches keyword '{keyword}' (head: {head_matches}, tail: {tail_matches})")
        
        return head_matches or tail_matches
    
    def _select_triplets_for_modify_and_insert(self, triplets, sentences, keywords, modify_count, insert_count):
        """
        Modifyì™€ Insertì— ì í•©í•œ triplet ì„ íƒ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        
        ê¸°ì¤€:
        - Modify: í‚¤ì›Œë“œ ê´€ë ¨ triplet ìš°ì„ 
        - Insert: ë‚˜ë¨¸ì§€ ëª¨ë“  triplet
        """
        if not triplets:
            return [], []
        
        # 1. í‚¤ì›Œë“œ ê´€ë ¨ triplet ë¶„ë¥˜
        keyword_related_triplets = []
        other_triplets = []
        
        for triple in triplets:
            is_keyword_related = any(self._is_triplet_related_to_keyword(triple, k) for k in keywords)
            
            if is_keyword_related:
                keyword_related_triplets.append(triple)
            else:
                other_triplets.append(triple)
        
        # 2. Modifyìš© triplet ì„ íƒ: í‚¤ì›Œë“œ ê´€ë ¨ triplet ìš°ì„ 
        modify_triplets = keyword_related_triplets[:modify_count]
        
        # ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ triplet ì¶”ê°€
        if len(modify_triplets) < modify_count and other_triplets:
            remaining = modify_count - len(modify_triplets)
            modify_triplets.extend(other_triplets[:remaining])
        
        # 3. Insertìš© triplet ì„ íƒ: ë‚˜ë¨¸ì§€ ëª¨ë“  triplet
        remaining_triplets = [t for t in triplets if t not in modify_triplets]
        insert_triplets = remaining_triplets[:insert_count]
        
        print(f"   ğŸ“Š Triplet classification: {len(keyword_related_triplets)} keyword-related, {len(other_triplets)} others")
        print(f"   ğŸ“Š Assigned: {len(modify_triplets)} for Modify, {len(insert_triplets)} for Insert")
        
        return modify_triplets, insert_triplets
    
    def _is_informative_triplet(self, triple):
        """Informativeí•œ ê´€ê³„ì¸ì§€ í™•ì¸"""
        head, relation, tail = triple
        
        # Informative relations
        informative_relations = [
            'located in', 'in', 'at', 'from', 'originated in',
            'has', 'contains', 'includes', 'features',
            'part of', 'belongs to', 'member of',
            'founded by', 'created by', 'established by',
            'born in', 'works for', 'employed by'
        ]
        
        relation_lower = relation.lower()
        return any(inf_rel in relation_lower for inf_rel in informative_relations)
    
    def _is_meaningful_triplet(self, triple, keywords):
        """ì˜ë¯¸ìˆëŠ” tripletì¸ì§€ í™•ì¸ (enhanced for better theme matching)"""
        head, relation, tail = triple
        
        # ì˜ë¯¸ìˆëŠ” ê´€ê³„ì¸ì§€ í™•ì¸
        meaningful_relations = [
            'is a', 'is an', 'instance of', 'type of', 'class of',
            'has', 'contains', 'includes', 'features',
            'part of', 'belongs to', 'member of', 'component of',
            'located in', 'in', 'at', 'situated in',
            'founded by', 'created by', 'established by', 'started by',
            'works for', 'employed by', 'works at',
            'born in', 'from', 'originated in',
            'developer', 'developed by', 'created by',
            'maker', 'manufacturer', 'producer'
        ]
        
        relation_lower = relation.lower()
        is_meaningful_relation = any(meaningful_rel in relation_lower for meaningful_rel in meaningful_relations)
        
        # í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ triplet ìš°ì„  ì„ íƒ
        is_keyword_related = False
        for keyword in keywords:
            if (self._is_entity_name_match(head, keyword) or 
                self._is_entity_name_match(tail, keyword)):
                is_keyword_related = True
                break
        
        # ë” ì—„ê²©í•˜ê²Œ: í‚¤ì›Œë“œ ê´€ë ¨ì´ê±°ë‚˜ ì˜ë¯¸ìˆëŠ” ê´€ê³„ì´ë©´ í—ˆìš©í•˜ë˜, í‚¤ì›Œë“œ ê´€ë ¨ triplet ìš°ì„ 
        return is_meaningful_relation or is_keyword_related
    
    def modify_sentences_with_keywords_and_triplets(self, sentences, triplets, keywords):
        """LLM-based sentence modification with triplet integration (RAG/CoT approach)"""
        if not triplets or not sentences or not keywords:
            return sentences, set()
        
        print(f"ğŸ“ Modifying sentences with RAG/CoT approach...")
        print(f"   Original sentences: {len(sentences)}")
        
        modified_sentences = []
        used_triplets = set()
        available_triplets = triplets.copy()
        modification_indices = []  # Track which sentences were modified
        
        for i, sentence in enumerate(sentences):
            # Find relevant triplet for this sentence
            best_triplet = self._find_relevant_triplet_for_sentence(sentence, available_triplets)
            
            if best_triplet:
                # LLM-based modification with RAG approach
                modified_sentence = self._llm_modify_sentence_with_triplet(sentence, best_triplet, keywords)
                
                if modified_sentence and modified_sentence != sentence:
                    # Only replace if successfully modified
                    modified_sentences.append(modified_sentence)
                    modification_indices.append(i)
                    used_triplets.add(tuple(best_triplet))
                    available_triplets.remove(best_triplet)
                    print(f"   âœ… Modified sentence {i+1}")
                else:
                    # Keep original if modification failed or same
                    modified_sentences.append(sentence)
                    print(f"   âšª Kept original sentence {i+1} (no modification or failed)")
            else:
                # Keep original if no triplet matched
                modified_sentences.append(sentence)
        
        print(f"   âœ… Modified {len(modification_indices)}/{len(sentences)} sentences")
        print(f"   Final sentences: {len(modified_sentences)} (should be {len(sentences)})")
        return modified_sentences, used_triplets
    
    def _llm_modify_sentence_with_triplet(self, sentence, triplet, keywords):
        """Use LLM to naturally integrate triplet into sentence (RAG approach)"""
        try:
            h, r, t = triplet
            h_name = self._select_best_name(h, self.entity)
            r_name = self._select_best_name(r, self.relation)
            t_name = self._select_best_name(t, self.entity)
            
            # RAG-style prompt for natural integration with grammar and proper noun preservation
            prompt = f"""Modify the following sentence to naturally include the given fact while preserving ALL original content and ensuring grammatical correctness.

Original sentence: {sentence}
Fact to integrate: ({h_name}, {r_name}, {t_name})

Document context: {', '.join(keywords)}

CRITICAL REQUIREMENTS:
- Preserve ALL original words and content from the original sentence
- Keep proper nouns capitalized correctly (e.g., Apple, California, New York)
- Maintain grammatical correctness and sentence flow
- The fact should be integrated smoothly without changing sentence structure
- Do not add unnecessary words or change core meaning
- Ensure the sentence sounds natural and professional
- Output ONLY the complete modified sentence, nothing else

Modified sentence:"""

            response = self.llm.generate(prompt, max_tokens=80, temperature=0.3)
            modified = response.strip().replace('Modified sentence:', '').strip() if response else ""
            
            # ë¹ˆ ì‘ë‹µ ì²´í¬ ë° fallback
            if not modified or len(modified) < 10:
                if not modified:
                    print(f"   âš ï¸  LLM returned empty response for modification, using fallback")
                # Fallback: ê°„ë‹¨í•œ ìˆ˜ì • ë°©ì‹ ì‚¬ìš©
                modified = self._simple_modify_sentence(sentence, triplet, keywords)
                if not modified or modified == sentence:
                    # Fallbackë„ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜
                    return sentence
            
            # Preserve proper nouns from original
            modified = self._preserve_proper_nouns(sentence, modified)
            
            # Quality check
            if len(modified) > 300:
                # ë„ˆë¬´ ê¸¸ë©´ fallback ì‚¬ìš©
                modified = self._simple_modify_sentence(sentence, triplet, keywords)
                if not modified or modified == sentence:
                    return sentence
            
            if not modified.endswith(('.', '!', '?')):
                modified += '.'
            
            return modified
            
        except Exception as e:
            print(f"   âš ï¸  LLM modification failed: {e}, using fallback")
            # Fallback: ê°„ë‹¨í•œ ìˆ˜ì • ë°©ì‹ ì‚¬ìš©
            try:
                modified = self._simple_modify_sentence(sentence, triplet, keywords)
                if modified and modified != sentence:
                    return modified
            except:
                pass
            return sentence
    
    def _preserve_proper_nouns(self, original, modified):
        """Preserve proper nouns from original sentence"""
        import re
        try:
            # Extract proper nouns from original (capitalized words)
            doc_original = self.nlp(original)
            proper_nouns = set()
            for token in doc_original:
                if token.text[0].isupper() and len(token.text) > 1:
                    proper_nouns.add(token.text)
            
            # Replace in modified if different
            doc_modified = self.nlp(modified)
            words = modified.split()
            result_words = []
            
            for word in words:
                # Remove punctuation for comparison
                clean_word = re.sub(r'[^\w]', '', word)
                if clean_word in proper_nouns:
                    # Preserve original capitalization
                    original_word = [w for w in proper_nouns if w.lower() == clean_word.lower()]
                    if original_word:
                        # Keep the original capitalization
                        if word[0].isupper():
                            result_words.append(original_word[0] + word[len(clean_word):])
                        else:
                            result_words.append(word)
                    else:
                        result_words.append(word)
                else:
                    result_words.append(word)
            
            return ' '.join(result_words)
        except:
            return modified
    
    def _find_relevant_triplet_for_sentence(self, sentence, triplets):
        """Find relevant triplet for a sentence using semantic similarity"""
        if not triplets:
            return None
        
        try:
            # Get sentence embedding
            sent_embed = self._get_sentence_embedding(sentence)
            if sent_embed is None:
                return triplets[0] if triplets else None
            
            best_triplet = None
            best_sim = 0.1  # Lower threshold to allow more matches
            
            for triplet in triplets:
                if len(triplet) >= 3:
                    # Create triplet text for embedding
                    triplet_text = f"{self._select_best_name(triplet[0], self.entity)} {self._select_best_name(triplet[1], self.relation)} {self._select_best_name(triplet[2], self.entity)}"
                    trip_embed = self._get_sentence_embedding(triplet_text)
                    
                    if trip_embed is not None:
                        sim = self._calculate_cosine_similarity(sent_embed, trip_embed)
                        if sim > best_sim:
                            best_sim = sim
                            best_triplet = triplet
            
            # Fallback: If no triplet found with similarity threshold, use first available triplet
            if best_triplet is None and triplets:
                print(f"   âš ï¸  No triplet found with similarity > 0.1, using first available triplet")
                return triplets[0]
            
            return best_triplet
        except Exception as e:
            return triplets[0] if triplets else None
    
    def _find_best_triplet_for_sentence(self, sentence, triplets, sentence_keywords):
        """ë¬¸ì¥ì— ê°€ì¥ ì í•©í•œ triplet ì°¾ê¸° (ë” ê´€ëŒ€í•œ ì¡°ê±´)"""
        if not triplets:
            return None
    
        best_triplet = None
        best_score = -1
        
        for triplet in triplets:
            if len(triplet) >= 3:
                head, relation, tail = triplet
                
                # ê¸°ë³¸ ì ìˆ˜ (ëª¨ë“  tripletì— ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬)
                score = 1
                
                # í‚¤ì›Œë“œì™€ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ìˆëŠ” ê²½ìš°ì—ë§Œ)
                if sentence_keywords:
                    for keyword in sentence_keywords:
                        if keyword.lower() in head.lower() or keyword.lower() in tail.lower():
                            score += 2
                        if keyword.lower() in relation.lower():
                            score += 1
                
                # ë¬¸ì¥ ë‚´ìš©ê³¼ì˜ ê´€ë ¨ì„± í™•ì¸ (ë” ê´€ëŒ€í•œ ë§¤ì¹­)
                sentence_lower = sentence.lower()
                if any(word in sentence_lower for word in head.lower().split()):
                    score += 1
                if any(word in sentence_lower for word in tail.lower().split()):
                    score += 1
                
                # ë¶€ë¶„ ë§¤ì¹­ë„ ê³ ë ¤
                if any(word in sentence_lower for word in head.lower().split() if len(word) > 3):
                    score += 0.5
                if any(word in sentence_lower for word in tail.lower().split() if len(word) > 3):
                    score += 0.5
                
                if score > best_score:
                    best_score = score
                    best_triplet = triplet
        
        # ë” ê´€ëŒ€í•œ ì¡°ê±´: ì ìˆ˜ê°€ 0.5 ì´ìƒì´ë©´ ì‚¬ìš©
        return best_triplet if best_score >= 0.5 else None
    
    def _simple_modify_sentence(self, sentence, triplet, keywords):
        """ê°„ë‹¨í•œ ë¬¸ì¥ ìˆ˜ì • (ê¸¸ì´ ì œí•œ)"""
        try:
            head, relation, tail = triplet
            
            # ì´ë¦„ ì„ íƒ
            head_name = self._select_best_name(head, self.entity)
            relation_name = self._select_best_name(relation, self.relation)
            tail_name = self._select_best_name(tail, self.entity)
            
            if not all([head_name, relation_name, tail_name]):
                return sentence
            
            # ê°„ë‹¨í•œ ì‚½ì… íŒ¨í„´ë“¤ (ê¸¸ì´ ì œí•œ)
            if relation_name.lower() in ['located', 'based', 'headquartered']:
                # ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
                if sentence.endswith('.'):
                    return sentence[:-1] + f", {relation_name} in {tail_name}."
                else:
                    return sentence + f", {relation_name} in {tail_name}."
            
            elif relation_name.lower() in ['founded', 'established', 'created']:
                # ì„¤ë¦½ ì •ë³´ ì¶”ê°€
                if sentence.endswith('.'):
                    return sentence[:-1] + f", {relation_name} in {tail_name}."
                else:
                    return sentence + f", {relation_name} in {tail_name}."
            
            elif relation_name.lower() in ['developer', 'producer', 'manufacturer']:
                # ê°œë°œ/ì œì¡° ì •ë³´ ì¶”ê°€
                if sentence.endswith('.'):
                    return sentence[:-1] + f" ({relation_name} by {tail_name})."
                else:
                    return sentence + f" ({relation_name} by {tail_name})."
            
            else:
                # ê¸°ë³¸ íŒ¨í„´
                if sentence.endswith('.'):
                    return sentence[:-1] + f" ({relation_name} {tail_name})."
                else:
                    return sentence + f" ({relation_name} {tail_name})."
                    
        except Exception as e:
            print(f"Error in simple sentence modification: {e}")
            return sentence
    
    def _llm_modify_sentences_with_context(self, sentences, triplets, keywords):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ë“¤ì„ ë¬¸ë§¥ì— ë§ê²Œ ì§€ëŠ¥ì ìœ¼ë¡œ ìˆ˜ì •"""
        try:
            # tripletë“¤ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
            triplet_sentences = []
            for triplet in triplets:
                sentence = self.convert_triplet_to_sentence_with_llm(triplet, keywords)
                triplet_sentences.append(sentence)
            
            # í˜„ì¬ ë¬¸ì„œì˜ ë¬¸ë§¥ ì •ë³´ ìˆ˜ì§‘
            context_info = self._analyze_document_context(sentences, keywords)
            
            # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_sentence_modification_prompt(sentences, triplet_sentences, context_info, keywords)
            
            # LLM í˜¸ì¶œ
            response = self.llm.generate(
                prompt, 
                max_tokens=600,
                temperature=0.6,
                do_sample=True
            )
            
            # ì‘ë‹µì—ì„œ ìˆ˜ì •ëœ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            modified_sentences = self._parse_llm_modification_response(response, sentences)
            
            # ì‚¬ìš©ëœ tripletë“¤ ì¶”ì  (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            used_triplets = set()
            for i, (orig, mod) in enumerate(zip(sentences, modified_sentences)):
                if orig != mod:
                    # ìˆ˜ì •ëœ ë¬¸ì¥ì— ì‚¬ìš©ëœ triplet ì°¾ê¸°
                    for j, triplet in enumerate(triplets):
                        if j < len(triplet_sentences):
                            triplet_text = triplet_sentences[j]
                            if any(word in mod.lower() for word in triplet_text.lower().split()[:3]):
                                used_triplets.add(tuple(triplet))
            
            print(f"   âœ… Successfully modified {len([i for i, (orig, mod) in enumerate(zip(sentences, modified_sentences)) if orig != mod])} sentences using LLM")
            return modified_sentences, used_triplets
                
        except Exception as e:
            print(f"   âŒ LLM modification failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë¬¸ì¥ ë°˜í™˜
            return sentences, set()
    
    def _create_sentence_modification_prompt(self, sentences, triplet_sentences, context_info, keywords):
        """ë¬¸ì¥ ìˆ˜ì •ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        sentences_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(sentences)])
        triplets_text = "\n".join([f"- {s}" for s in triplet_sentences])
        
        prompt = f"""Enhance these sentences by naturally integrating relevant facts while preserving original meaning.

        SENTENCES:
        {sentences_text}

        FACTS TO INTEGRATE:
        {triplets_text}

        TOPICS: {', '.join(keywords)}

        RULES:
        - Keep original meaning intact
        - Add relevant facts naturally using connectors like "which", "that", "additionally"
        - Only modify sentences where facts fit naturally
        - Keep additions concise (max 15 words per addition)
        - Maintain consistent tone

        MODIFIED SENTENCES:"""
        
        return prompt
    
    def _parse_llm_modification_response(self, response, original_sentences):
        """LLM ìˆ˜ì • ì‘ë‹µì—ì„œ ë¬¸ì¥ë“¤ì„ ì¶”ì¶œ"""
        try:
            # ì‘ë‹µì—ì„œ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            lines = response.strip().split('\n')
            modified_sentences = []
            
            for line in lines:
                line = line.strip()
                # ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë¬¸ì¥ íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: "1. ë¬¸ì¥ë‚´ìš©")
                if re.match(r'^\d+\.\s+', line):
                    sentence = re.sub(r'^\d+\.\s+', '', line)
                    if sentence and len(sentence) > 5:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                        modified_sentences.append(sentence)
            
            # ì¶”ì¶œëœ ë¬¸ì¥ ìˆ˜ê°€ ì›ë³¸ê³¼ ë‹¤ë¥´ë©´ ì›ë³¸ ë°˜í™˜
            if len(modified_sentences) != len(original_sentences):
                print(f"   âš ï¸  LLM response parsing incomplete, using original sentences")
                return original_sentences
            
            return modified_sentences
            
        except Exception as e:
            print(f"   âš ï¸  Error parsing LLM modification response: {e}")
            return original_sentences
    
    def _calculate_triplet_sentence_similarity(self, triplet, sentence):
        """Tripletì˜ head, relation, tailê³¼ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        head, relation, tail = triplet
        
        # ê° êµ¬ì„±ìš”ì†Œì˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        head_name = self._select_best_name(head, self.entity)
        relation_name = self._select_best_name(relation, self.relation)
        tail_name = self._select_best_name(tail, self.entity)
        
        # ê° êµ¬ì„±ìš”ì†Œì™€ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        head_embedding = self._get_sentence_embedding(head_name)
        relation_embedding = self._get_sentence_embedding(relation_name)
        tail_embedding = self._get_sentence_embedding(tail_name)
        sentence_embedding = self._get_sentence_embedding(sentence)
        
        if not all([head_embedding is not None, relation_embedding is not None, 
                   tail_embedding is not None, sentence_embedding is not None]):
            return 0.0
        
        # ê° êµ¬ì„±ìš”ì†Œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° í›„ í‰ê· 
        head_sim = self._calculate_cosine_similarity(head_embedding, sentence_embedding)
        relation_sim = self._calculate_cosine_similarity(relation_embedding, sentence_embedding)
        tail_sim = self._calculate_cosine_similarity(tail_embedding, sentence_embedding)
        
        # ê°€ì¤‘ í‰ê·  (headì™€ tailì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        avg_similarity = (head_sim * 0.4 + relation_sim * 0.2 + tail_sim * 0.4)
        return avg_similarity
    
    
    
    def convert_triplet_to_sentence_with_llm(self, triplet, keywords=None):
        """RAG ìŠ¤íƒ€ì¼ë¡œ tripletì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜"""
        if not isinstance(triplet, (list, tuple)) or len(triplet) < 3:
            return str(triplet)
        
        head, relation, tail = triplet
        
        # ê° êµ¬ì„±ìš”ì†Œì˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        head_name = self._select_best_name(head, self.entity, keywords)
        relation_name = self._select_best_name(relation, self.relation)
        tail_name = self._select_best_name(tail, self.entity, keywords)
        
        # RAG ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""Convert this knowledge into a natural, informative sentence.

SUBJECT: {head_name}
RELATION: {relation_name}
OBJECT: {tail_name}

CONTEXT: Document about {', '.join(keywords) if keywords else 'general topics'}

Create a single, grammatically correct sentence (max 20 words) that sounds natural and professional.

SENTENCE:"""

        try:
            # LLM í˜¸ì¶œ
            response = self.llm.generate(
                prompt,
                max_tokens=40,
                temperature=0.7,
                do_sample=True
            )
            
            # ì‘ë‹µì—ì„œ ë¬¸ì¥ ì¶”ì¶œ ë° ì •ë¦¬
            generated_sentence = response.strip() if response else ""
            
            # ë¹ˆ ì‘ë‹µ ë˜ëŠ” ê¸°ë³¸ í’ˆì§ˆ ê²€ì‚¬ (ê¸¸ì´ ì œí•œ ê°•í™”)
            if not generated_sentence or len(generated_sentence) < 10 or len(generated_sentence) > 150:
                if not generated_sentence:
                    print(f"   âš ï¸  LLM returned empty response, using fallback")
                return self._create_fallback_sentence(head_name, relation_name, tail_name)
            
            # ë‹¨ì–´ ìˆ˜ ì œí•œ (20ë‹¨ì–´ ì´í•˜)
            word_count = len(generated_sentence.split())
            if word_count > 20:
                # 20ë‹¨ì–´ë¡œ ìë¥´ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë§ˆë¬´ë¦¬
                words = generated_sentence.split()[:20]
                generated_sentence = ' '.join(words)
                if not generated_sentence.endswith(('.', '!', '?')):
                    generated_sentence += '.'
            
            # ë¬¸ì¥ì´ ì œëŒ€ë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
            if not generated_sentence.endswith(('.', '!', '?')):
                generated_sentence += '.'
            
            return generated_sentence
            
        except Exception as e:
            print(f"   Error in LLM triplet conversion: {e}")
            return self._create_fallback_sentence(head_name, relation_name, tail_name)
    
    def _create_fallback_sentence(self, head_name, relation_name, tail_name):
        """LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ë¬¸ì¥ íŒ¨í„´ (ê°„ê²° ë²„ì „)"""
        relation_lower = relation_name.lower()
        
        # ì´ë¦„ ê¸¸ì´ ì œí•œ (10ì ì´í•˜)
        if len(head_name) > 10:
            head_name = head_name[:10] + "..."
        if len(tail_name) > 10:
            tail_name = tail_name[:10] + "..."
        
        # ê¸°ë³¸ íŒ¨í„´ë“¤ (ê°„ê²°í•˜ê²Œ)
        if 'is a' in relation_lower or 'instance of' in relation_lower:
            sentence = f"{head_name} is a {tail_name}."
        elif 'has' in relation_lower or 'contains' in relation_lower:
            sentence = f"{head_name} has {tail_name}."
        elif 'located in' in relation_lower or 'in' in relation_lower:
            sentence = f"{head_name} is in {tail_name}."
        elif 'founded by' in relation_lower or 'created by' in relation_lower:
            sentence = f"{head_name} was founded by {tail_name}."
        elif 'part of' in relation_lower:
            sentence = f"{head_name} is part of {tail_name}."
        else:
            sentence = f"{head_name} {relation_name} {tail_name}."
        
        # ìµœì¢… ê¸¸ì´ ì œí•œ (50ì ì´í•˜)
        if len(sentence) > 50:
            sentence = f"{head_name} is related to {tail_name}."
        
        return sentence
    
    def _is_unnatural_sentence(self, sentence):
        """ë¬¸ì¥ì´ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ì§€ íŒë‹¨ (ë™ì  ë°©ë²•)"""
        if not sentence or len(sentence) < 5:
            return True
        
        # 1. ë¬¸ë²•ì  êµ¬ì¡° ê²€ì‚¬
        if not self._has_valid_grammatical_structure(sentence):
            return True
        
        # 2. ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë‚˜ êµ¬ë¬¸ ê²€ì‚¬
        if self._has_repetitive_patterns(sentence):
            return True
        
        # 3. ì˜ë¯¸ì  ì¼ê´€ì„± ê²€ì‚¬
        if not self._has_semantic_coherence(sentence):
            return True
        
        # 4. ë¬¸ì¥ ê¸¸ì´ì™€ ë³µì¡ë„ ê²€ì‚¬
        if not self._has_appropriate_length_and_complexity(sentence):
            return True
        
        return False
    
    def _has_valid_grammatical_structure(self, sentence):
        """ë¬¸ë²•ì  êµ¬ì¡°ê°€ ìœ íš¨í•œì§€ ê²€ì‚¬"""
        try:
            doc = self.nlp(sentence)
            
            # ë¬¸ì¥ì´ ì œëŒ€ë¡œ ì‹œì‘í•˜ëŠ”ì§€
            if not sentence[0].isupper():
                return False
            
            # ì£¼ì–´ì™€ ë™ì‚¬ê°€ ìˆëŠ”ì§€
            has_subject = any(token.dep_ in ['nsubj', 'nsubjpass'] for token in doc)
            has_verb = any(token.pos_ == 'VERB' for token in doc)
            
            if not has_subject or not has_verb:
                return False
            
            # ë„ˆë¬´ ë§ì€ íŠ¹ìˆ˜ë¬¸ì
            special_chars = sum(1 for c in sentence if not c.isalnum() and c != ' ' and c != '.' and c != ',' and c != '!' and c != '?')
            if special_chars > len(sentence) * 0.15:  # 15% ì´ìƒì´ íŠ¹ìˆ˜ë¬¸ì
                return False
            
            return True
        except:
            return False
    
    def _has_repetitive_patterns(self, sentence):
        """ë°˜ë³µë˜ëŠ” íŒ¨í„´ì´ ìˆëŠ”ì§€ ê²€ì‚¬"""
        words = sentence.lower().split()
        
        # ê°™ì€ ë‹¨ì–´ê°€ 3ë²ˆ ì´ìƒ ë°˜ë³µ
        word_counts = {}
        for word in words:
            if len(word) > 2:  # 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
                word_counts[word] = word_counts.get(word, 0) + 1
                if word_counts[word] >= 3:
                    return True
        
        # ì—°ì†ëœ ê°™ì€ ë‹¨ì–´ (ì˜ˆ: "apple apple apple")
        for i in range(len(words) - 2):
            if words[i] == words[i+1] == words[i+2]:
                return True
        
        # ë¹„ìŠ·í•œ ë‹¨ì–´ê°€ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” íŒ¨í„´ (ì˜ˆ: "california state california")
        for i in range(len(words) - 2):
            if words[i] == words[i+2] and words[i+1] in ['state', 'is', 'a', 'an', 'the']:
                return True
        
        return False
    
    def _has_semantic_coherence(self, sentence):
        """ì˜ë¯¸ì  ì¼ê´€ì„±ì´ ìˆëŠ”ì§€ ê²€ì‚¬"""
        try:
            doc = self.nlp(sentence)
            
            # ëª…ì‚¬ì™€ ë™ì‚¬ì˜ ê´€ê³„ê°€ ì ì ˆí•œì§€
            nouns = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN']]
            verbs = [token.text for token in doc if token.pos_ == 'VERB']
            
            # ëª…ì‚¬ê°€ ë„ˆë¬´ ë§ê³  ë™ì‚¬ê°€ ì ìœ¼ë©´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€
            if len(nouns) > 5 and len(verbs) < 2:
                return False
            
            # ì˜ë¯¸ê°€ ëª¨í˜¸í•œ ë‹¨ì–´ ì¡°í•© ê²€ì‚¬ (ì¼ë°˜ì ì¸ ë¬¸ë²• ì˜¤ë¥˜)
            sentence_lower = sentence.lower()
            ambiguous_patterns = [
                'is a lists',  # ë¬¸ë²• ì˜¤ë¥˜
                'type of technolog',  # ì² ì ì˜¤ë¥˜
                'pull media company',  # ì˜ë¯¸ ëª¨í˜¸
            ]
            
            for pattern in ambiguous_patterns:
                if pattern in sentence_lower:
                    return False
            
            return True
        except:
            return True  # ì—ëŸ¬ ì‹œ í†µê³¼
    
    def _has_appropriate_length_and_complexity(self, sentence):
        """ì ì ˆí•œ ê¸¸ì´ì™€ ë³µì¡ë„ë¥¼ ê°€ì§€ëŠ”ì§€ ê²€ì‚¬"""
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥
        if len(sentence) < 10 or len(sentence) > 300:
            return False
        
        # ë‹¨ì–´ ìˆ˜ê°€ ì ì ˆí•œì§€
        word_count = len(sentence.split())
        if word_count < 3 or word_count > 50:
            return False
        
        # ë¬¸ì¥ì´ ì œëŒ€ë¡œ ëë‚˜ëŠ”ì§€
        if not sentence.rstrip().endswith(('.', '!', '?')):
            return False
        
        return True
    
    def _verify_and_fix_naturalness(self, sentences, original_sentences, keywords):
        """ë¬¸ì¥ë“¤ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì„ ê²€ì¦í•˜ê³  ìˆ˜ì • (ì›ë³¸ ë¬¸ì¥ë§Œ ë˜ëŒë¦¬ê¸°, ìƒˆë¡œ ì‚½ì…ëœ ë¬¸ì¥ì€ ìˆ˜ì •)"""
        if not sentences:
            return sentences
        
        print(f"ğŸ” Verifying naturalness of {len(sentences)} sentences...")
        
        verified_sentences = []
        original_set = set(s.strip().lower() for s in original_sentences) if original_sentences else set()
        unnatural_count = 0
        fixed_count = 0
        reverted_count = 0
        
        for i, sentence in enumerate(sentences):
            is_unnatural = self._is_unnatural_sentence(sentence)
            
            if is_unnatural:
                unnatural_count += 1
                sentence_normalized = sentence.strip().lower()
                
                # ì›ë³¸ ë¬¸ì¥ì´ê³  ì¸ë±ìŠ¤ê°€ ì›ë³¸ ë²”ìœ„ ë‚´ì¸ ê²½ìš°ë§Œ ì›ë³¸ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                if i < len(original_sentences) and sentence_normalized in original_set:
                    # ì›ë³¸ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¬¸ì¥ ì°¾ê¸°
                    found_original = False
                    for orig in original_sentences:
                        if orig.strip().lower() == sentence_normalized:
                            verified_sentences.append(orig)
                            reverted_count += 1
                            print(f"   ğŸ”„ Reverted to original (sentence {i+1}): {sentence[:50]}...")
                            found_original = True
                            break
                    
                    if not found_original:
                        # ì¸ë±ìŠ¤ë¡œ ì›ë³¸ ë¬¸ì¥ ê°€ì ¸ì˜¤ê¸°
                        verified_sentences.append(original_sentences[i])
                        reverted_count += 1
                        print(f"   ğŸ”„ Reverted to original by index (sentence {i+1})")
                else:
                    # ìƒˆë¡œ ì‚½ì…ëœ ë¬¸ì¥ì´ê±°ë‚˜ ì›ë³¸ì´ ì•„ë‹Œ ê²½ìš° LLMìœ¼ë¡œ ìˆ˜ì • ì‹œë„
                    fixed = self._fix_unnatural_sentence_with_llm(sentence, keywords)
                    if fixed and not self._is_unnatural_sentence(fixed):
                        verified_sentences.append(fixed)
                        fixed_count += 1
                        print(f"   âœ… Fixed with LLM (sentence {i+1}): {sentence[:50]}... â†’ {fixed[:50]}...")
                    else:
                        # ìˆ˜ì • ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€ (ì‚½ì…ëœ ë¬¸ì¥ì€ ìœ ì§€)
                        verified_sentences.append(sentence)
                        print(f"   âš ï¸  Could not fix (sentence {i+1}), keeping as-is")
            else:
                # ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                verified_sentences.append(sentence)
        
        if unnatural_count > 0:
            print(f"   ğŸ“Š Naturalness verification results:")
            print(f"      - Unnatural sentences detected: {unnatural_count}")
            print(f"      - Fixed with LLM: {fixed_count}")
            print(f"      - Reverted to original: {reverted_count}")
            print(f"      - Remaining unnatural: {unnatural_count - fixed_count - reverted_count}")
        else:
            print(f"   âœ… All sentences passed naturalness verification")
        
        return verified_sentences
    
    def _fix_unnatural_sentence_with_llm(self, sentence, keywords):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ìˆ˜ì •"""
        try:
            prompt = f"""Fix the following sentence to make it more natural and grammatically correct while preserving its core meaning.

Sentence to fix: {sentence}

Document context: {', '.join(keywords) if keywords else 'general topics'}

REQUIREMENTS:
- Fix grammatical errors and awkward phrasing
- Make the sentence sound natural and professional
- Preserve the core meaning and key information
- Ensure proper capitalization and punctuation
- Output ONLY the corrected sentence, nothing else

Corrected sentence:"""

            response = self.llm.generate(prompt, max_tokens=60, temperature=0.3)
            fixed = response.strip() if response else ""
            
            # ë¹ˆ ì‘ë‹µ ë˜ëŠ” í’ˆì§ˆ ê²€ì‚¬
            if not fixed or len(fixed) < 5:
                return None
            
            # ë„ˆë¬´ ê¸´ ê²½ìš°ë„ ì œì™¸
            if len(fixed) > 300:
                return None
            
            # ë¬¸ì¥ ë ë§ˆì¹¨í‘œ í™•ì¸
            if not fixed.endswith(('.', '!', '?')):
                fixed += '.'
            
            return fixed
            
        except Exception as e:
            print(f"   âš ï¸  Error fixing sentence with LLM: {e}")
            return None
    
    def _fix_grammar_with_llm(self, sentences):
        """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¬¸ë²• ìˆ˜ì • (LLM ëŒ€ì‹ )"""
        if not sentences:
            return sentences
        
        corrected_sentences = []
        
        for sentence in sentences:
            try:
                corrected = sentence
                
                # 1. ê´€ì‚¬ ì˜¤ë¥˜ ìˆ˜ì • (a/an)
                import re
                # "an" + ììŒìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ â†’ "a"
                corrected = re.sub(r'\ban\s+([bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ])', r'a \1', corrected, flags=re.IGNORECASE)
                
                # "a" + ëª¨ìŒìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ â†’ "an" (ë‹¨, ëŒ€ë¬¸ì ë‹¨ì–´ëŠ” ì œì™¸)
                corrected = re.sub(r'\ba\s+([aeiouAEIOU][a-z])', r'an \1', corrected)
                
                # 2. ëŒ€ë¬¸ì ë¬¸ì œ ìˆ˜ì • (NER í™œìš©)
                try:
                    # ê¸°ì¡´ self.nlpë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ëª…ì‚¬ ì‹ë³„
                    doc = self.nlp(corrected)
                    
                    # NER ì—”í‹°í‹°ë“¤ì„ ì¶”ì¶œ
                    entities = set()
                    for ent in doc.ents:
                        # ì—”í‹°í‹°ì˜ ê° ë‹¨ì–´ë¥¼ ì¶”ê°€
                        for token in ent.text.split():
                            entities.add(token.strip())
                    
                    words = corrected.split()
                    fixed_words = []
                    
                    for i, word in enumerate(words):
                        # ì „ì²´ ëŒ€ë¬¸ì ë‹¨ì–´ ì²˜ë¦¬
                        if word.isupper() and len(word) > 2:
                            # NERì—ì„œ ì‹ë³„ëœ ì—”í‹°í‹°ì¸ì§€ í™•ì¸
                            if word in entities:
                                # ê³ ìœ ëª…ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                                fixed_words.append(word)
                            elif i == 0:
                                # ë¬¸ì¥ ì²« ë‹¨ì–´ëŠ” ì²« ê¸€ìë§Œ ëŒ€ë¬¸ì
                                fixed_words.append(word.capitalize())
                            else:
                                # ë¬¸ì¥ ì¤‘ê°„ì˜ ì¼ë°˜ ë‹¨ì–´ëŠ” ì†Œë¬¸ìí™”
                                fixed_words.append(word.lower())
                        # ë¶€ë¶„ ëŒ€ë¬¸ì ë‹¨ì–´ (ì˜ˆ: "SOFTWARE", "Linux")
                        elif word.isupper() and len(word) > 2 and word.lower() not in ['i', 'a', 'an', 'the']:
                            # NERì—ì„œ ì‹ë³„ëœ ì—”í‹°í‹°ì¸ì§€ í™•ì¸
                            if word in entities:
                                # ê³ ìœ ëª…ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                                fixed_words.append(word)
                            elif i == 0:
                                # ë¬¸ì¥ ì²« ë‹¨ì–´ëŠ” ì²« ê¸€ìë§Œ ëŒ€ë¬¸ì
                                fixed_words.append(word.capitalize())
                            else:
                                # ë¬¸ì¥ ì¤‘ê°„ì˜ ì¼ë°˜ ë‹¨ì–´ëŠ” ì†Œë¬¸ìí™”
                                fixed_words.append(word.lower())
                        else:
                            fixed_words.append(word)
                            
                except Exception as e:
                    # NER ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê·œì¹™ ì ìš©
                    print(f"   âš ï¸  NER failed, using basic rules: {e}")
                    words = corrected.split()
                    fixed_words = []
                    
                    for i, word in enumerate(words):
                        if word.isupper() and len(word) > 2:
                            if i == 0:
                                fixed_words.append(word.capitalize())
                            else:
                                fixed_words.append(word.lower())
                        else:
                            fixed_words.append(word)
                
                corrected = ' '.join(fixed_words)
                
                # 3. ì¤‘ë³µ ê³µë°± ì œê±°
                corrected = re.sub(r'\s+', ' ', corrected)
                
                # 4. ë¬¸ì¥ ë ë§ˆì¹¨í‘œ í™•ì¸
                corrected = corrected.strip()
                if corrected and not corrected.endswith(('.', '!', '?')):
                    corrected += '.'
                
                corrected_sentences.append(corrected)
                
                if corrected != sentence:
                    print(f"   âœ… Fixed: {sentence[:30]}... â†’ {corrected[:30]}...")
                    
            except Exception as e:
                corrected_sentences.append(sentence)
                print(f"   âš ï¸  Error fixing sentence: {e}")
        
        print(f"   âœ… Grammar fixed {len([s for s in corrected_sentences if s != sentences[corrected_sentences.index(s)]])} sentences")
        return corrected_sentences
    
    def _remove_duplicate_sentences_preserving_originals(self, watermarked_sentences, original_sentences):
        """ì¤‘ë³µ ë¬¸ì¥ ì œê±° - ì›ë³¸ ë¬¸ì¥ë“¤ì€ ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŒ"""
        if not watermarked_sentences:
            return watermarked_sentences
        
        # ì›ë³¸ ë¬¸ì¥ë“¤ì˜ ìˆ˜ì • ë²„ì „ë„ ì¶”ì 
        # ì‹¤ì œë¡œëŠ” ëª¨ë“  ë¬¸ì¥ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì „
        
        seen = set()
        unique_sentences = []
        duplicates_removed = 0
        
        for sentence in watermarked_sentences:
            normalized = sentence.strip().lower()
            
            if normalized not in seen:
                unique_sentences.append(sentence)
                seen.add(normalized)
            else:
                # ì¤‘ë³µ ë°œê²¬
                duplicates_removed += 1
                print(f"   ğŸ—‘ï¸  Removed duplicate (attempt to preserve original content)")
        
        if duplicates_removed > 0:
            print(f"   âœ… Removed {duplicates_removed} duplicate sentences")
            print(f"   Final: {len(unique_sentences)} unique sentences (original: {len(original_sentences)})")
        else:
            print(f"   âœ“ No duplicates found")
        
        return unique_sentences
    
    def _remove_duplicate_sentences(self, sentences):
        """ì¤‘ë³µ ë¬¸ì¥ ì œê±° (ìˆœì„œ ìœ ì§€) - legacy"""
        if not sentences:
            return sentences
        
        seen = set()
        unique_sentences = []
        duplicates_removed = 0
        
        for sentence in sentences:
            # ë¬¸ì¥ì„ ì •ê·œí™”í•˜ì—¬ ë¹„êµ (ê³µë°±, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            normalized = sentence.strip().lower()
            
            if normalized not in seen:
                seen.add(normalized)
                unique_sentences.append(sentence)
            else:
                duplicates_removed += 1
                print(f"   ğŸ—‘ï¸  Removed duplicate: {sentence[:50]}...")
        
        if duplicates_removed > 0:
            print(f"   âœ… Removed {duplicates_removed} duplicate sentences")
        
        return unique_sentences
    
    def insert_sentences_at_appropriate_positions(self, sentences, triplets, keywords):
        """Insert sentences using LLM with context-aware bridge generation (CoT approach)"""
        if not triplets or not sentences:
            return sentences
        
        print(f"â• Inserting {len(triplets)} triplets with context-aware bridging...")
        print(f"   Original sentences: {len(sentences)}")
        
        # Start with all original sentences
        result_sentences = sentences.copy()
        original_count = len(sentences)
        
        for i, triplet in enumerate(triplets):
            if len(triplet) >= 3:
                # Find best insertion position
                position = self._find_best_insertion_position(result_sentences, triplet, keywords)
                
                # Generate bridge sentence with context
                bridge_sentence = self._llm_generate_bridge_sentence(
                    result_sentences, position, triplet, keywords
                )
                
                if bridge_sentence and bridge_sentence.strip():
                    result_sentences.insert(position, bridge_sentence)
                    print(f"   âœ… Inserted triplet {i+1} at position {position}: {bridge_sentence[:50]}...")
                else:
                    print(f"   âš ï¸  Skipped triplet {i+1} (empty or None bridge sentence)")
        
        print(f"   Final sentences: {len(result_sentences)} (original: {original_count}, inserted: {len(result_sentences) - original_count})")
        return result_sentences
    
    def _llm_generate_bridge_sentence(self, sentences, position, triplet, keywords):
        """Generate natural bridge sentence between context (CoT approach)"""
        try:
            h, r, t = triplet
            h_name = self._select_best_name(h, self.entity)
            r_name = self._select_best_name(r, self.relation)
            t_name = self._select_best_name(t, self.entity)
            
            # Get context
            prev_context = " ".join(sentences[max(0, position-2):position]) if position > 0 else ""
            next_context = " ".join(sentences[position:min(len(sentences), position+2)]) if position < len(sentences) else ""
            
            # CoT-style prompt for bridge generation with grammar preservation
            prompt = f"""Generate a grammatically correct connecting sentence that smoothly bridges between two paragraphs while incorporating the given fact.

Previous context: {prev_context if prev_context else '(beginning of document)'}
Next context: {next_context if next_context else '(end of document)'}
Fact to incorporate: ({h_name}, {r_name}, {t_name})

Document context: {', '.join(keywords)}

REQUIREMENTS:
- Create a natural transition sentence (15-25 words)
- Ensure proper grammar and sentence structure
- Capitalize proper nouns correctly ({h_name}, {t_name} should keep their original capitalization)
- Use appropriate connectors (Furthermore, Additionally, Moreover, etc.)
- Make sure the sentence sounds professional and natural
- The sentence should connect previous and next contexts smoothly
- Incorporate the fact naturally as part of the sentence
- Output ONLY the bridge sentence, nothing else

Bridge sentence:"""

            response = self.llm.generate(prompt, max_tokens=50, temperature=0.5)
            bridge = response.strip().replace('Bridge sentence:', '').strip() if response else ""
            
            # ë¹ˆ ì‘ë‹µ ì²´í¬ ë° fallback
            if not bridge or len(bridge) < 10:
                if not bridge:
                    print(f"   âš ï¸  LLM returned empty response for bridge sentence, using fallback")
                # Fallback: ê°„ë‹¨í•œ triplet ë¬¸ì¥ ìƒì„±
                bridge = self._create_simple_triplet_sentence(triplet, keywords)
                if not bridge:
                    return None
            
            # Quality check
            if len(bridge) > 150:
                return None
            
            if not bridge.endswith(('.', '!', '?')):
                bridge += '.'
            
            # Preserve capitalization of proper nouns
            if h_name:
                bridge = re.sub(r'\b' + re.escape(h_name.lower()) + r'\b', h_name, bridge, flags=re.IGNORECASE)
            if t_name:
                bridge = re.sub(r'\b' + re.escape(t_name.lower()) + r'\b', t_name, bridge, flags=re.IGNORECASE)
            
            return bridge
            
        except Exception as e:
            print(f"   âš ï¸  Bridge generation failed: {e}, using fallback")
            # Fallback: ê°„ë‹¨í•œ triplet ë¬¸ì¥ ìƒì„±
            try:
                bridge = self._create_simple_triplet_sentence(triplet, keywords)
                return bridge
            except:
                return None
    
    def _find_best_insertion_position(self, sentences, triplet, keywords):
        """Find best position to insert sentence based on semantic similarity"""
        if not sentences:
            return 0
        
        # Try to find semantically relevant position
        triplet_text = f"{self._select_best_name(triplet[0], self.entity)} {self._select_best_name(triplet[1], self.relation)} {self._select_best_name(triplet[2], self.entity)}"
        
        try:
            triplet_embedding = self._get_sentence_embedding(triplet_text)
            if triplet_embedding is not None:
                best_pos = 0
                best_sim = -1
                
                for i in range(len(sentences)):
                    sent_embed = self._get_sentence_embedding(sentences[i])
                    if sent_embed is not None:
                        sim = self._calculate_cosine_similarity(triplet_embedding, sent_embed)
                        if sim > best_sim:
                            best_sim = sim
                            best_pos = i + 1
                
                return best_pos if best_sim > 0.2 else len(sentences) // 2
        except:
            pass
        
        # Default to middle
        return len(sentences) // 2
    
    def _create_simple_triplet_sentence(self, triplet, keywords):
        """ê°„ë‹¨í•œ triplet ë¬¸ì¥ ìƒì„± (ê¸¸ì´ ì œí•œ)"""
        try:
            head, relation, tail = triplet
            
            # ì´ë¦„ ì„ íƒ
            head_name = self._select_best_name(head, self.entity)
            relation_name = self._select_best_name(relation, self.relation)
            tail_name = self._select_best_name(tail, self.entity)
            
            if not all([head_name, relation_name, tail_name]):
                return None
            
            # ê°„ë‹¨í•œ ë¬¸ì¥ íŒ¨í„´ë“¤
            if relation_name.lower() in ['located', 'based', 'headquartered']:
                return f"{head_name} is {relation_name} in {tail_name}."
            elif relation_name.lower() in ['founded', 'established', 'created']:
                return f"{head_name} was {relation_name} in {tail_name}."
            elif relation_name.lower() in ['developer', 'producer', 'manufacturer']:
                return f"{head_name} is a {relation_name} of {tail_name}."
            elif relation_name.lower() in ['part', 'member', 'component']:
                return f"{head_name} is part of {tail_name}."
            else:
                return f"{head_name} {relation_name} {tail_name}."
                
        except Exception as e:
            print(f"Error creating simple triplet sentence: {e}")
            return None
    
    def _find_simple_insert_position(self, sentences, triplet):
        """ê°„ë‹¨í•œ ì‚½ì… ìœ„ì¹˜ ì°¾ê¸°"""
        if not sentences:
            return 0
        
        # ë¬¸ì¥ ì¤‘ê°„ ìœ„ì¹˜ì— ì‚½ì… (ë„ˆë¬´ ì•ì´ë‚˜ ë’¤ëŠ” í”¼í•¨)
        if len(sentences) <= 2:
            return len(sentences)  # ë§¨ ë’¤ì— ì‚½ì…
        else:
            # ì¤‘ê°„ ìœ„ì¹˜ë“¤ ì¤‘ì—ì„œ ì„ íƒ
            middle_positions = [len(sentences) // 2, len(sentences) // 2 + 1]
            return middle_positions[0]  # ì²« ë²ˆì§¸ ì¤‘ê°„ ìœ„ì¹˜
    
    def _llm_integrate_triplets_with_context(self, sentences, triplets, keywords):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ tripletë“¤ì„ ë¬¸ë§¥ì— ë§ê²Œ ì§€ëŠ¥ì ìœ¼ë¡œ í†µí•©"""
        try:
            # tripletë“¤ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
            triplet_sentences = []
            for triplet in triplets:
                sentence = self.convert_triplet_to_sentence_with_llm(triplet, keywords)
                triplet_sentences.append(sentence)
            
            # í˜„ì¬ ë¬¸ì„œì˜ ë¬¸ë§¥ ì •ë³´ ìˆ˜ì§‘
            context_info = self._analyze_document_context(sentences, keywords)
            
            # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_context_integration_prompt(sentences, triplet_sentences, context_info, keywords)
            
            # LLM í˜¸ì¶œ
            response = self.llm.generate(
                prompt,
                max_tokens=500,
                temperature=0.7,
                do_sample=True
            )
            
            # ì‘ë‹µì—ì„œ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            integrated_sentences = self._parse_llm_response(response, sentences)
            
            print(f"   âœ… Successfully integrated {len(triplet_sentences)} triplets using LLM")
            return integrated_sentences
            
        except Exception as e:
            print(f"   âŒ LLM integration failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ fallback
            return self._fallback_integration(sentences, triplets, keywords)
    
    def _analyze_document_context(self, sentences, keywords):
        """ë¬¸ì„œì˜ ë¬¸ë§¥ ì •ë³´ ë¶„ì„"""
        context = {
            'total_sentences': len(sentences),
            'avg_sentence_length': sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0,
            'document_style': 'formal',
            'main_topics': keywords,
            'sentence_types': []
        }
        
        # ê°„ë‹¨í•œ ë¬¸ì¥ ìœ í˜• ë¶„ì„ (LLMì´ ë” ì •í™•í•˜ê²Œ ì²˜ë¦¬)
        for sentence in sentences:
            # ê¸°ë³¸ì ì¸ ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜
            if any(word in sentence.lower() for word in ['founded', 'established', 'created', 'developed']):
                context['sentence_types'].append('factual')
            elif any(word in sentence.lower() for word in ['is a', 'are a', 'was a', 'were a']):
                context['sentence_types'].append('descriptive')
            else:
                context['sentence_types'].append('general')
        
        return context
    
    def _create_context_integration_prompt(self, sentences, triplet_sentences, context_info, keywords):
        """ë¬¸ë§¥ í†µí•©ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        sentences_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(sentences)])
        triplets_text = "\n".join([f"- {s}" for s in triplet_sentences])
        
        prompt = f"""Integrate new factual information into this document naturally while maintaining flow and coherence.

DOCUMENT:
{sentences_text}

NEW FACTS:
{triplets_text}

TOPICS: {', '.join(keywords)}

INTEGRATION:
- Place facts near related content
- Use natural connectors ("Additionally", "Furthermore", "Moreover")
- Keep new sentences concise (max 25 words each)
- Maintain document flow and readability
- Number each sentence in output

INTEGRATED DOCUMENT:"""

        return prompt
    
    def _parse_llm_response(self, response, original_sentences):
        """LLM ì‘ë‹µì—ì„œ ë¬¸ì¥ë“¤ì„ ì¶”ì¶œ"""
        try:
            # ì‘ë‹µì—ì„œ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            lines = response.strip().split('\n')
            integrated_sentences = []
            
            for line in lines:
                line = line.strip()
                # ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë¬¸ì¥ íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: "1. ë¬¸ì¥ë‚´ìš©")
                if re.match(r'^\d+\.\s+', line):
                    sentence = re.sub(r'^\d+\.\s+', '', line)
                    if sentence and len(sentence) > 5:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                        integrated_sentences.append(sentence)
            
            # ì¶”ì¶œëœ ë¬¸ì¥ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ + ìƒˆ ë¬¸ì¥ìœ¼ë¡œ fallback
            if len(integrated_sentences) < len(original_sentences) * 0.8:
                print(f"   âš ï¸  LLM response parsing incomplete, using fallback")
                return original_sentences
            
            return integrated_sentences
            
        except Exception as e:
            print(f"   âš ï¸  Error parsing LLM response: {e}")
            return original_sentences
    
    def _fallback_integration(self, sentences, triplets, keywords):
        """LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ í†µí•© ë°©ì‹"""
        result_sentences = sentences.copy()
        
        for triplet in triplets:
            sentence = self.convert_triplet_to_sentence_with_llm(triplet, keywords)
            if sentence:
                # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê¸°ë°˜ ì‚½ì…
                best_position = self._find_best_insertion_position_with_similarity(result_sentences, sentence)
                result_sentences.insert(best_position, sentence)
        
        return result_sentences
    
    
    def _find_best_insertion_position_with_similarity(self, sentences, triplet_sentence):
        """Semantic similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì‚½ì… ìœ„ì¹˜ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)"""
        if not sentences:
            return 0
        
        # RoBERTaë¥¼ ì‚¬ìš©í•œ semantic similarity ê³„ì‚°
        triplet_embedding = self._get_sentence_embedding(triplet_sentence)
        if triplet_embedding is None:
            # RoBERTa ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ ìœ„ì¹˜ì— ì‚½ì…
            return len(sentences) // 2
        
        best_position = 0
        best_similarity = -1.0
        
        # ê° ë¬¸ì¥ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        for i, sentence in enumerate(sentences):
            sentence_embedding = self._get_sentence_embedding(sentence)
            if sentence_embedding is None:
                continue
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self._calculate_cosine_similarity(triplet_embedding, sentence_embedding)
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_position = i + 1  # ë¬¸ì¥ ë’¤ì— ì‚½ì…
        
        # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¤‘ê°„ ìœ„ì¹˜ì— ì‚½ì…
        if best_similarity < 0.1:
            return len(sentences) // 2
        
        print(f"   Best similarity: {best_similarity:.3f} at position {best_position}")
        return best_position
    
    def _verify_triplet_entity_preservation(self, text: str, triplets: List[List[str]]) -> Dict:
        """
        Verify that Head and Tail entities of triplets are preserved in the text
        
        Args:
            text: Watermarked text to verify
            triplets: List of triplets to verify
        
        Returns:
            Dictionary mapping triplet tuples to verification results
        """
        verification_results = {}
        
        # Split text into sentences
        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]
        
        for triplet in triplets:
            if len(triplet) < 3:
                continue
            
            head_id, relation_id, tail_id = triplet[0], triplet[1], triplet[2]
            triplet_tuple = tuple(triplet)
            
            # Get entity names
            head_names = self._get_entity_names_for_verification(head_id)
            tail_names = self._get_entity_names_for_verification(tail_id)
            
            # Check if both head and tail are found in the same sentence
            head_found = False
            tail_found = False
            both_found = False
            
            for sentence in sentences:
                sentence_lower = sentence.lower()
                
                # Check head entity
                head_matched = False
                for head_name in head_names:
                    if head_name and isinstance(head_name, str):
                        if head_name.lower() in sentence_lower:
                            head_found = True
                            head_matched = True
                            break
                
                # Check tail entity
                tail_matched = False
                for tail_name in tail_names:
                    if tail_name and isinstance(tail_name, str):
                        if tail_name.lower() in sentence_lower:
                            tail_found = True
                            tail_matched = True
                            break
                
                # If both found in same sentence
                if head_matched and tail_matched:
                    both_found = True
                    break
            
            verification_results[triplet_tuple] = {
                "head_found": head_found,
                "tail_found": tail_found,
                "both_found": both_found,
                "head_names": head_names,
                "tail_names": tail_names
            }
        
        return verification_results
    
    def _get_entity_names_for_verification(self, entity_id: str) -> List[str]:
        """Get all entity names for verification (similar to watermark_detection)"""
        names = []
        if entity_id in self.entity:
            entity_data = self.entity[entity_id]
            if isinstance(entity_data, dict):
                names.extend(entity_data.get("entity", []))
            elif isinstance(entity_data, list):
                names.extend(entity_data)
        
        # If no names found, use entity_id as fallback
        if not names:
            names = [str(entity_id)]
        
        # Filter to only English strings
        english_names = [name for name in names if isinstance(name, str) and self._is_english_text(name)]
        return english_names if english_names else [name for name in names if isinstance(name, str)]
    
    def _retry_insert_failed_triplets(self, sentences: List[str], failed_triplets: List[List[str]], 
                                     keywords: List[str]) -> Optional[List[str]]:
        """
        Retry inserting failed triplets with more explicit entity name preservation
        
        Args:
            sentences: Current watermarked sentences
            failed_triplets: Triplets that failed verification
            keywords: Document keywords
        
        Returns:
            Updated sentences with retried triplets, or None if failed
        """
        if not failed_triplets:
            return sentences
        
        result_sentences = sentences.copy()
        
        for triplet in failed_triplets:
            if len(triplet) < 3:
                continue
            
            head_id, relation_id, tail_id = triplet[0], triplet[1], triplet[2]
            
            # Get entity names explicitly
            head_names = self._get_entity_names_for_verification(head_id)
            tail_names = self._get_entity_names_for_verification(tail_id)
            
            # Select best names (prefer shorter, more common names)
            head_name = head_names[0] if head_names else str(head_id)
            tail_name = tail_names[0] if tail_names else str(tail_id)
            relation_name = self._select_best_name(relation_id, self.relation)
            
            # Generate sentence with explicit entity names
            try:
                # More explicit prompt to ensure entity names are preserved
                prompt = f"""Create a natural sentence that explicitly includes the following entities and their relationship.

Entity 1 (Head): {head_name}
Relation: {relation_name}
Entity 2 (Tail): {tail_name}

Document context: {', '.join(keywords)}

REQUIREMENTS:
- MUST include both "{head_name}" and "{tail_name}" in the sentence
- Use the exact entity names provided (do not paraphrase or replace them)
- Create a grammatically correct sentence (15-25 words)
- Make it sound natural and professional
- Output ONLY the sentence, nothing else

Sentence:"""

                response = self.llm.generate(prompt, max_tokens=50, temperature=0.3)
                new_sentence = response.strip()
                
                # Verify entity names are in the generated sentence
                new_sentence_lower = new_sentence.lower()
                head_in_sentence = head_name.lower() in new_sentence_lower
                tail_in_sentence = tail_name.lower() in new_sentence_lower
                
                if head_in_sentence and tail_in_sentence:
                    # Quality check
                    if len(new_sentence) < 10 or len(new_sentence) > 200:
                        continue
                    
                    if not new_sentence.endswith(('.', '!', '?')):
                        new_sentence += '.'
                    
                    # Insert at appropriate position
                    position = self._find_best_insertion_position(result_sentences, triplet, keywords)
                    result_sentences.insert(position, new_sentence)
                    print(f"      âœ… Retried triplet: {head_name} ... {tail_name}")
                else:
                    print(f"      âš ï¸  Retry failed: entities not preserved in generated sentence")
                    
            except Exception as e:
                print(f"      âš ï¸  Retry insertion error: {e}")
                continue
        
        return result_sentences if len(result_sentences) > len(sentences) else None